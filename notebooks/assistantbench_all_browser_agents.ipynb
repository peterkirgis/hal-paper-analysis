{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Process ALL AssistantBench browser agent files\nimport os, json, zipfile, io, tempfile, re, base64\nfrom typing import Iterator, Dict, Any, Optional, List, Tuple\nfrom huggingface_hub import HfFileSystem\nfrom tqdm import tqdm\nimport ijson\nfrom cryptography.fernet import Fernet\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\n\nfrom pydantic import Field\n\nfrom docent.data_models import AgentRun, Transcript\nfrom docent.data_models.chat import ChatMessage, ToolCall, parse_chat_message\nfrom docent import Docent\n\nREPO_ID  = \"agent-evals/hal_traces\"\nREVISION = \"main\"\n\n# Set token from environment variable\nhf_token = os.getenv(\"HF_TOKEN\")\nif not hf_token:\n    raise ValueError(\"HF_TOKEN environment variable is required\")\nos.environ[\"HF_TOKEN\"] = hf_token"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIXED helper functions loaded - now preserves all message roles including empty content!\n"
     ]
    }
   ],
   "source": [
    "# Helper functions from your working demo (FIXED to include all roles)\n",
    "_CODE_FENCE_RE = re.compile(r\"^\\s*```[\\w+-]*\\n(.*?)\\n```$\", re.DOTALL)\n",
    "\n",
    "def _canon_text(s: str) -> str:\n",
    "    \"\"\"Strip code fences, normalize whitespace so identical answers hash the same.\"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = s.strip()\n",
    "    m = _CODE_FENCE_RE.match(s)\n",
    "    if m:\n",
    "        s = m.group(1)\n",
    "    lines = [ln.rstrip() for ln in s.splitlines()]\n",
    "    out, prev_blank = [], False\n",
    "    for ln in lines:\n",
    "        blank = (ln == \"\")\n",
    "        if blank and prev_blank:\n",
    "            continue\n",
    "        out.append(ln)\n",
    "        prev_blank = blank\n",
    "    return \"\\n\".join(out).strip()\n",
    "\n",
    "def _tc_canon(tc):\n",
    "    typ = (tc.get(\"type\") or \"function\")\n",
    "    fn  = tc.get(\"function\")\n",
    "    if isinstance(fn, dict):\n",
    "        fn = fn.get(\"name\")\n",
    "    if fn is None:\n",
    "        fn = \"\"\n",
    "    args = tc.get(\"arguments\", {})\n",
    "    if isinstance(args, (dict, list)):\n",
    "        args_canon = json.dumps(args, sort_keys=True, separators=(\",\", \":\"), default=str)\n",
    "    else:\n",
    "        args_canon = str(args)\n",
    "    return (typ, str(fn), args_canon)\n",
    "\n",
    "def _msg_fingerprint(m: Dict[str, Any]) -> tuple:\n",
    "    \"\"\"Fingerprint of a normalized chat message.\"\"\"\n",
    "    role = m.get(\"role\")\n",
    "    content = _canon_text(m.get(\"content\") or \"\")\n",
    "    tool_calls = tuple(sorted(_tc_canon(tc) for tc in (m.get(\"tool_calls\") or []) if isinstance(tc, dict)))\n",
    "    return (role, content, tool_calls)\n",
    "\n",
    "def dedupe_messages(messages, mode: str = \"consecutive\"):\n",
    "    \"\"\"Drop duplicate messages.\"\"\"\n",
    "    out = []\n",
    "    last_fp = None\n",
    "    seen = set()\n",
    "    for m in messages:\n",
    "        fp = _msg_fingerprint(m)\n",
    "        if mode == \"consecutive\":\n",
    "            if fp == last_fp:\n",
    "                continue\n",
    "            last_fp = fp\n",
    "        else:  # global\n",
    "            if fp in seen:\n",
    "                continue\n",
    "            seen.add(fp)\n",
    "        out.append(m)\n",
    "    return out\n",
    "\n",
    "def _collapse_content_to_text(content) -> str:\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "    if isinstance(content, list):\n",
    "        parts = []\n",
    "        for seg in content:\n",
    "            if isinstance(seg, dict) and isinstance(seg.get(\"text\"), str):\n",
    "                parts.append(seg[\"text\"])\n",
    "        return \"\\n\".join(p for p in parts if p)\n",
    "    return str(content)\n",
    "\n",
    "def _map_role(raw_type, raw__type):\n",
    "    t = (raw_type or raw__type or \"\").lower()\n",
    "    if t in (\"ai\", \"assistant\"): return \"assistant\"\n",
    "    if t in (\"human\", \"user\"):   return \"user\"\n",
    "    if t == \"system\":            return \"system\"\n",
    "    return None\n",
    "\n",
    "def normalize_weave_log_item(item: Dict[str, Any]):\n",
    "    \"\"\"Prefer the 'inputs.raw' single-message rows.\"\"\"\n",
    "    raw = item.get(\"inputs\", {}).get(\"raw\")\n",
    "    if not isinstance(raw, dict):\n",
    "        return None\n",
    "    role = _map_role(raw.get(\"type\"), raw.get(\"_type\"))\n",
    "    if role is None:\n",
    "        return None\n",
    "    content_text = _collapse_content_to_text(raw.get(\"content\"))\n",
    "    tool_calls = raw.get(\"tool_calls\") or []\n",
    "    ts = item.get(\"started_at\") or item.get(\"created_timestamp\")\n",
    "    return {\n",
    "        \"role\": role,\n",
    "        \"content\": content_text,\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"id\": tc.get(\"id\"),\n",
    "                \"function\": (tc.get(\"name\") or (tc.get(\"function\") or {}).get(\"name\")),\n",
    "                \"arguments\": (tc.get(\"args\")  or (tc.get(\"function\") or {}).get(\"arguments\", {})),\n",
    "                \"type\": tc.get(\"type\") or \"function\",\n",
    "            }\n",
    "            for tc in tool_calls if isinstance(tc, dict)\n",
    "        ],\n",
    "        \"ts\": ts,\n",
    "    }\n",
    "\n",
    "def normalize_assistant_output(item: Dict[str, Any]):\n",
    "    \"\"\"Pick up OpenAI-style assistant messages from the 'output' side.\"\"\"\n",
    "    out = item.get(\"output\") or {}\n",
    "    choices = out.get(\"choices\") or []\n",
    "    if not choices:\n",
    "        return None\n",
    "    msg = choices[0].get(\"message\") or {}\n",
    "    content = msg.get(\"content\")\n",
    "    if not content:\n",
    "        return None\n",
    "    return {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": content if isinstance(content, str) else _collapse_content_to_text(content),\n",
    "        \"tool_calls\": [],\n",
    "        \"ts\": item.get(\"ended_at\") or item.get(\"created_timestamp\"),\n",
    "    }\n",
    "\n",
    "def _maybe_take_initial_system_msgs(item: Dict[str, Any]):\n",
    "    \"\"\"Capture SYSTEM messages from inputs.messages once per task.\"\"\"\n",
    "    msgs = item.get(\"inputs\", {}).get(\"messages\")\n",
    "    out = []\n",
    "    if isinstance(msgs, list):\n",
    "        for m in msgs:\n",
    "            if isinstance(m, dict) and m.get(\"role\") == \"system\":\n",
    "                out.append({\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": _collapse_content_to_text(m.get(\"content\")),\n",
    "                    \"tool_calls\": [],\n",
    "                    \"ts\": item.get(\"started_at\") or item.get(\"created_timestamp\"),\n",
    "                })\n",
    "    return out\n",
    "\n",
    "def _build_agent_run_from_bucket(tid: str, bucket, model, eval_blob=None):\n",
    "    # FIXED: Include messages with empty content (important for system/user messages)\n",
    "    # Old version filtered: [m for m in bucket if m.get(\"role\") and m.get(\"content\") is not None]\n",
    "    # New version includes all messages with a role, even if content is empty\n",
    "    msgs_sorted = sorted([m for m in bucket if m.get(\"role\")], \n",
    "                         key=lambda m: m.get(\"ts\") or \"\")\n",
    "    \n",
    "    # strip ts from final output\n",
    "    final_messages = []\n",
    "    for m in msgs_sorted:\n",
    "        mm = {\"role\": m[\"role\"], \"content\": m.get(\"content\") or \"\"}  # Default to empty string\n",
    "        if m.get(\"tool_calls\"):\n",
    "            mm[\"tool_calls\"] = m[\"tool_calls\"]\n",
    "        final_messages.append(mm)\n",
    "\n",
    "    agent_run = {\n",
    "        \"weave_task_id\": tid,\n",
    "        \"model\": model,\n",
    "        \"messages\": final_messages,\n",
    "    }\n",
    "    if eval_blob:\n",
    "        agent_run[\"eval\"] = {\n",
    "            \"reward\": eval_blob.get(\"reward\"),\n",
    "            \"task\": eval_blob.get(\"task\", eval_blob.get(\"info\", {})) or {},\n",
    "        }\n",
    "    \n",
    "    agent_run[\"messages\"] = dedupe_messages(agent_run[\"messages\"])\n",
    "    return agent_run\n",
    "\n",
    "print(\"FIXED helper functions loaded - now preserves all message roles including empty content!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIXED message processing - now extracts contextual messages per item for proper ordering!\n"
     ]
    }
   ],
   "source": [
    "# Decryption functions (FIXED for proper message ordering)\n",
    "def _derive_key(password: str, salt: bytes) -> bytes:\n",
    "    kdf = PBKDF2HMAC(algorithm=hashes.SHA256(), length=32, salt=salt, iterations=480000)\n",
    "    return base64.urlsafe_b64encode(kdf.derive(password.encode()))\n",
    "\n",
    "def decrypt_token_bytes(encrypted_data_b64: str, salt_b64: str, password: str = \"hal1234\") -> bytes:\n",
    "    ct = base64.b64decode(encrypted_data_b64)\n",
    "    salt = base64.b64decode(salt_b64)\n",
    "    f = Fernet(_derive_key(password, salt))\n",
    "    return f.decrypt(ct)\n",
    "\n",
    "def _decrypt_container_to_tempfile(container: Dict[str, Any]) -> str:\n",
    "    plaintext = decrypt_token_bytes(container[\"encrypted_data\"], container[\"salt\"])\n",
    "    tf = tempfile.NamedTemporaryFile(delete=False, suffix=\".json\")\n",
    "    try:\n",
    "        tf.write(plaintext)\n",
    "        tf.flush()\n",
    "        return tf.name\n",
    "    finally:\n",
    "        tf.close()\n",
    "\n",
    "def extract_contextual_messages_from_item(item: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extract user, system, and assistant messages that are contextual to this specific log item\"\"\"\n",
    "    messages = []\n",
    "    ts = item.get(\"started_at\") or item.get(\"created_timestamp\")\n",
    "    \n",
    "    # Check inputs.messages for contextual user/system messages\n",
    "    input_messages = item.get(\"inputs\", {}).get(\"messages\", [])\n",
    "    if isinstance(input_messages, list):\n",
    "        for msg in input_messages:\n",
    "            if isinstance(msg, dict):\n",
    "                role = msg.get(\"role\")\n",
    "                if role in [\"user\", \"system\"]:\n",
    "                    content = _collapse_content_to_text(msg.get(\"content\"))\n",
    "                    # Include the message if it has content OR if it's a system message\n",
    "                    if content or role == \"system\":\n",
    "                        messages.append({\n",
    "                            \"role\": role,\n",
    "                            \"content\": content,\n",
    "                            \"tool_calls\": [],\n",
    "                            \"ts\": ts\n",
    "                        })\n",
    "    \n",
    "    # Also check for assistant messages in inputs.messages (some might be there)\n",
    "    for msg in input_messages:\n",
    "        if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\":\n",
    "            content = _collapse_content_to_text(msg.get(\"content\"))\n",
    "            if content:  # Only include assistant messages with content\n",
    "                messages.append({\n",
    "                    \"role\": \"assistant\", \n",
    "                    \"content\": content,\n",
    "                    \"tool_calls\": msg.get(\"tool_calls\", []),\n",
    "                    \"ts\": ts\n",
    "                })\n",
    "    \n",
    "    return messages\n",
    "\n",
    "def stream_agent_runs_by_task(repo_id: str, zip_name: str, *, revision: str = \"main\", \n",
    "                             repo_kind: str = \"datasets\", member_name=None,\n",
    "                             require_model=None, include_eval: bool = False, \n",
    "                             limit=None, aggregate_all: bool = True):\n",
    "    # Open ZIP from HF\n",
    "    fs = HfFileSystem()\n",
    "    zip_path = f\"{repo_kind}/{repo_id}@{revision}/{zip_name}\"\n",
    "    hf_file = fs.open(zip_path, \"rb\")\n",
    "    zf = zipfile.ZipFile(hf_file)\n",
    "    if member_name:\n",
    "        info = zf.getinfo(member_name)\n",
    "    else:\n",
    "        info = next(i for i in zf.infolist() if not i.filename.endswith(\"/\"))\n",
    "    \n",
    "    try:\n",
    "        with zf.open(info, \"r\") as member:\n",
    "            container = json.load(member)\n",
    "    finally:\n",
    "        try: zf.close()\n",
    "        except: pass\n",
    "        try: hf_file.close()\n",
    "        except: pass\n",
    "\n",
    "    plaintext_path = _decrypt_container_to_tempfile(container)\n",
    "\n",
    "    tasks_bucket = {}\n",
    "    model_by_tid = {}\n",
    "    eval_by_tid = {}\n",
    "    produced = 0\n",
    "    \n",
    "    try:\n",
    "        with open(plaintext_path, \"rb\") as f:\n",
    "            for item in ijson.items(f, \"raw_logging_results.item\"):\n",
    "                tid = item.get(\"weave_task_id\")\n",
    "                if not tid:\n",
    "                    continue\n",
    "\n",
    "                # Model discovery\n",
    "                mdl = (item.get(\"inputs\", {}) or {}).get(\"model\") or (item.get(\"output\", {}) or {}).get(\"model\")\n",
    "                if mdl:\n",
    "                    model_by_tid[tid] = mdl\n",
    "\n",
    "                # Eval blob\n",
    "                if (\"reward\" in item) or (\"task\" in item) or (\"info\" in item):\n",
    "                    eval_by_tid[tid] = item\n",
    "                    continue\n",
    "\n",
    "                # Filter by model if requested\n",
    "                if require_model and ((item.get(\"inputs\", {}) or {}).get(\"model\") != require_model):\n",
    "                    continue\n",
    "\n",
    "                # Start bucket\n",
    "                if tid not in tasks_bucket:\n",
    "                    tasks_bucket[tid] = []\n",
    "\n",
    "                # FIXED: Extract ALL contextual messages from this item (user, system, assistant)\n",
    "                # This ensures proper timestamps and ordering\n",
    "                contextual_messages = extract_contextual_messages_from_item(item)\n",
    "                tasks_bucket[tid].extend(contextual_messages)\n",
    "\n",
    "                # EXISTING: Normalize individual messages from inputs.raw\n",
    "                nm = normalize_weave_log_item(item)\n",
    "                if nm:\n",
    "                    tasks_bucket[tid].append(nm)\n",
    "\n",
    "                # EXISTING: Assistant messages from outputs\n",
    "                ao = normalize_assistant_output(item)\n",
    "                if ao:\n",
    "                    tasks_bucket[tid].append(ao)\n",
    "\n",
    "        # Emit once per task\n",
    "        for tid, bucket in tasks_bucket.items():\n",
    "            run = _build_agent_run_from_bucket(\n",
    "                tid=tid,\n",
    "                bucket=bucket,\n",
    "                model=model_by_tid.get(tid),\n",
    "                eval_blob=eval_by_tid.get(tid) if include_eval else None,\n",
    "            )\n",
    "            yield tid, run\n",
    "            produced += 1\n",
    "            if limit and produced >= limit:\n",
    "                break\n",
    "    finally:\n",
    "        try: os.remove(plaintext_path)\n",
    "        except OSError: pass\n",
    "\n",
    "print(\"FIXED message processing - now extracts contextual messages per item for proper ordering!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert loaded_results to docent ChatMessage format\n",
    "def normalize_message_for_docent(msg):\n",
    "    \"\"\"Normalize AssistantBench message format to docent-compatible format\"\"\"\n",
    "    normalized = msg.copy()\n",
    "    \n",
    "    # Fix tool_calls format if present\n",
    "    if 'tool_calls' in normalized and normalized['tool_calls']:\n",
    "        fixed_tool_calls = []\n",
    "        for tool_call in normalized['tool_calls']:\n",
    "            fixed_tc = {}\n",
    "            \n",
    "            # Required fields for docent ToolCall\n",
    "            fixed_tc['id'] = tool_call.get('id', f\"tool_{len(fixed_tool_calls)}\")  # Ensure ID exists\n",
    "            fixed_tc['type'] = 'function'  # docent expects 'function'\n",
    "            \n",
    "            # Function name - should be a string, not a dict\n",
    "            if isinstance(tool_call.get('function'), str):\n",
    "                fixed_tc['function'] = tool_call['function']\n",
    "            elif isinstance(tool_call.get('function'), dict):\n",
    "                # If it's a dict with 'name', use that\n",
    "                fixed_tc['function'] = tool_call['function'].get('name', 'unknown_function')\n",
    "            else:\n",
    "                fixed_tc['function'] = 'unknown_function'\n",
    "            \n",
    "            # Arguments - should be a dict at the top level\n",
    "            if 'arguments' in tool_call:\n",
    "                fixed_tc['arguments'] = tool_call['arguments']\n",
    "            elif isinstance(tool_call.get('function'), dict) and 'arguments' in tool_call['function']:\n",
    "                fixed_tc['arguments'] = tool_call['function']['arguments']\n",
    "            else:\n",
    "                fixed_tc['arguments'] = {}\n",
    "            \n",
    "            fixed_tool_calls.append(fixed_tc)\n",
    "        \n",
    "        normalized['tool_calls'] = fixed_tool_calls\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def convert_to_docent_messages(loaded_results):\n",
    "    \"\"\"Convert AssistantBench results to docent ChatMessage format\"\"\"\n",
    "    docent_results = {}\n",
    "    \n",
    "    print(\"Converting to docent ChatMessage format...\")\n",
    "    conversion_stats = {\n",
    "        'total_messages': 0,\n",
    "        'successful': 0,\n",
    "        'failed': 0,\n",
    "        'failed_tasks': [],\n",
    "        'error_types': {}\n",
    "    }\n",
    "    \n",
    "    for zip_name, tasks in loaded_results.items():\n",
    "        if \"error\" in tasks:\n",
    "            docent_results[zip_name] = tasks  # Keep error info\n",
    "            continue\n",
    "            \n",
    "        docent_results[zip_name] = {}\n",
    "        \n",
    "        for task_id, agent_run in tasks.items():\n",
    "            messages = agent_run.get('messages', [])\n",
    "            conversion_stats['total_messages'] += len(messages)\n",
    "            \n",
    "            # Convert each message to docent ChatMessage\n",
    "            docent_messages = []\n",
    "            task_failed_count = 0\n",
    "            \n",
    "            for i, msg in enumerate(messages):\n",
    "                try:\n",
    "                    # Normalize the message format first\n",
    "                    normalized_msg = normalize_message_for_docent(msg)\n",
    "                    \n",
    "                    # Parse using docent's parse_chat_message function\n",
    "                    chat_msg = parse_chat_message(normalized_msg)\n",
    "                    docent_messages.append(chat_msg)\n",
    "                    conversion_stats['successful'] += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_type = type(e).__name__\n",
    "                    conversion_stats['error_types'][error_type] = conversion_stats['error_types'].get(error_type, 0) + 1\n",
    "                    \n",
    "                    # Only print first few errors to avoid spam\n",
    "                    if conversion_stats['failed'] < 5:\n",
    "                        print(f\"Warning: Failed to parse message {i} in task {task_id[:12]}...: {e}\")\n",
    "                    \n",
    "                    conversion_stats['failed'] += 1\n",
    "                    task_failed_count += 1\n",
    "                    continue\n",
    "            \n",
    "            if task_failed_count > 0:\n",
    "                conversion_stats['failed_tasks'].append((task_id, task_failed_count))\n",
    "            \n",
    "            # Store the converted data\n",
    "            docent_results[zip_name][task_id] = {\n",
    "                'weave_task_id': agent_run.get('weave_task_id'),\n",
    "                'model': agent_run.get('model'),\n",
    "                'eval': agent_run.get('eval'),\n",
    "                'original_message_count': len(messages),\n",
    "                'docent_message_count': len(docent_messages),\n",
    "                'failed_message_count': task_failed_count,\n",
    "                'docent_messages': docent_messages,  # These are now ChatMessage objects\n",
    "                'original_messages': messages  # Keep original for reference\n",
    "            }\n",
    "    \n",
    "    return docent_results, conversion_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_all_transcripts_to_transluce(docent_results, collection_id, client, batch_by_model=True):\n",
    "    \"\"\"\n",
    "    Upload all transcripts from docent_results to a transluce collection.\n",
    "    \n",
    "    Args:\n",
    "        docent_results: Dictionary containing converted transcript data\n",
    "        collection_id: The collection ID in transluce to upload to\n",
    "        client: The docent client instance\n",
    "        batch_by_model: If True, upload runs one model at a time instead of all at once\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with upload statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    upload_stats = {\n",
    "        'total_runs': 0,\n",
    "        'successful_uploads': 0,\n",
    "        'failed_uploads': 0,\n",
    "        'skipped_runs': 0,\n",
    "        'failed_runs': []\n",
    "    }\n",
    "    \n",
    "    print(\"üöÄ Processing docent_results for upload to transluce...\")\n",
    "    \n",
    "    # Group agent runs by model if batch_by_model is True\n",
    "    if batch_by_model:\n",
    "        model_groups = {}\n",
    "        \n",
    "        for zip_name, tasks in docent_results.items():\n",
    "            # Skip entries with errors\n",
    "            if \"error\" in tasks:\n",
    "                print(f\"‚ö†Ô∏è  Skipping {zip_name}: contains error\")\n",
    "                upload_stats['skipped_runs'] += 1\n",
    "                continue\n",
    "                \n",
    "            for task_id, agent_run_data in tasks.items():\n",
    "                model = agent_run_data.get('model', 'unknown')\n",
    "                if model not in model_groups:\n",
    "                    model_groups[model] = []\n",
    "                model_groups[model].append((zip_name, task_id, agent_run_data))\n",
    "        \n",
    "        print(f\"üìä Found {len(model_groups)} models:\")\n",
    "        for model, runs in model_groups.items():\n",
    "            print(f\"   {model}: {len(runs)} runs\")\n",
    "        \n",
    "        # Process each model group separately\n",
    "        for model, runs in model_groups.items():\n",
    "            print(f\"\\nüîÑ Processing model: {model} ({len(runs)} runs)\")\n",
    "            agent_runs = []\n",
    "            \n",
    "            for zip_name, task_id, agent_run_data in runs:\n",
    "                upload_stats['total_runs'] += 1\n",
    "                \n",
    "                try:\n",
    "                    # Extract metadata\n",
    "                    metadata = {\n",
    "                        \"benchmark_id\": \"assistantbench\",\n",
    "                        \"task_id\": task_id,\n",
    "                        \"model\": agent_run_data.get('model', 'unknown'),\n",
    "                        \"run_id\": zip_name,\n",
    "                        \"weave_task_id\": agent_run_data.get('weave_task_id'),\n",
    "                        \"eval\": agent_run_data.get('eval'),\n",
    "                        \"original_message_count\": agent_run_data['original_message_count'],\n",
    "                        \"docent_message_count\": agent_run_data['docent_message_count'],\n",
    "                        \"failed_message_count\": agent_run_data['failed_message_count']\n",
    "                    }\n",
    "                    \n",
    "                    # Create transcript from docent messages\n",
    "                    transcript = Transcript(\n",
    "                        messages=agent_run_data['docent_messages'],\n",
    "                        metadata=metadata\n",
    "                    )\n",
    "                    \n",
    "                    # Create transcripts dict (AgentRun expects plural)\n",
    "                    transcripts = {\n",
    "                        \"default\": transcript\n",
    "                    }\n",
    "                    \n",
    "                    # Create AgentRun\n",
    "                    agent_run = AgentRun(\n",
    "                        transcripts=transcripts,\n",
    "                        metadata=metadata\n",
    "                    )\n",
    "                    \n",
    "                    agent_runs.append(agent_run)\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Failed to create AgentRun for task {task_id[:12]}...: {e}\")\n",
    "                    upload_stats['failed_uploads'] += 1\n",
    "                    upload_stats['failed_runs'].append({\n",
    "                        'task_id': task_id,\n",
    "                        'zip_name': zip_name,\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "                    continue\n",
    "            \n",
    "            # Upload this model's runs\n",
    "            if agent_runs:\n",
    "                try:\n",
    "                    print(f\"   üîÑ Uploading {len(agent_runs)} runs for {model}...\")\n",
    "                    client.add_agent_runs(collection_id, agent_runs)\n",
    "                    upload_stats['successful_uploads'] += len(agent_runs)\n",
    "                    print(f\"   ‚úÖ Successfully uploaded {len(agent_runs)} runs for {model}!\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ùå Failed to upload runs for {model}: {e}\")\n",
    "                    upload_stats['failed_uploads'] += len(agent_runs)\n",
    "                    for agent_run in agent_runs:\n",
    "                        upload_stats['failed_runs'].append({\n",
    "                            'agent_run_id': agent_run.id,\n",
    "                            'model': model,\n",
    "                            'error': str(e)\n",
    "                        })\n",
    "    \n",
    "    else:\n",
    "        # Original behavior - upload all at once\n",
    "        agent_runs = []\n",
    "        \n",
    "        for zip_name, tasks in docent_results.items():\n",
    "            # Skip entries with errors\n",
    "            if \"error\" in tasks:\n",
    "                print(f\"‚ö†Ô∏è  Skipping {zip_name}: contains error\")\n",
    "                upload_stats['skipped_runs'] += 1\n",
    "                continue\n",
    "                \n",
    "            print(f\"üìÅ Processing {zip_name}...\")\n",
    "            \n",
    "            for task_id, agent_run_data in tasks.items():\n",
    "                upload_stats['total_runs'] += 1\n",
    "                \n",
    "                try:\n",
    "                    # Extract metadata\n",
    "                    metadata = {\n",
    "                        \"benchmark_id\": \"assistantbench\",\n",
    "                        \"task_id\": task_id,\n",
    "                        \"model\": agent_run_data.get('model', 'unknown'),\n",
    "                        \"run_id\": zip_name,\n",
    "                        \"weave_task_id\": agent_run_data.get('weave_task_id'),\n",
    "                        \"eval\": agent_run_data.get('eval'),\n",
    "                        \"original_message_count\": agent_run_data['original_message_count'],\n",
    "                        \"docent_message_count\": agent_run_data['docent_message_count'],\n",
    "                        \"failed_message_count\": agent_run_data['failed_message_count']\n",
    "                    }\n",
    "                    \n",
    "                    # Create transcript from docent messages\n",
    "                    transcript = Transcript(\n",
    "                        messages=agent_run_data['docent_messages'],\n",
    "                        metadata=metadata\n",
    "                    )\n",
    "                    \n",
    "                    # Create transcripts dict (AgentRun expects plural)\n",
    "                    transcripts = {\n",
    "                        \"default\": transcript\n",
    "                    }\n",
    "                    \n",
    "                    # Create AgentRun\n",
    "                    agent_run = AgentRun(\n",
    "                        transcripts=transcripts,\n",
    "                        metadata=metadata\n",
    "                    )\n",
    "                    \n",
    "                    agent_runs.append(agent_run)\n",
    "                    \n",
    "                    if len(agent_runs) % 10 == 0:\n",
    "                        print(f\"   ‚úÖ Prepared {len(agent_runs)} agent runs...\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Failed to create AgentRun for task {task_id[:12]}...: {e}\")\n",
    "                    upload_stats['failed_uploads'] += 1\n",
    "                    upload_stats['failed_runs'].append({\n",
    "                        'task_id': task_id,\n",
    "                        'zip_name': zip_name,\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "                    continue\n",
    "        \n",
    "        print(f\"üìä Prepared {len(agent_runs)} agent runs for upload\")\n",
    "        \n",
    "        # Upload all agent runs to the collection\n",
    "        if agent_runs:\n",
    "            try:\n",
    "                print(f\"üîÑ Uploading {len(agent_runs)} agent runs to collection {collection_id}...\")\n",
    "                client.add_agent_runs(collection_id, agent_runs)\n",
    "                upload_stats['successful_uploads'] = len(agent_runs)\n",
    "                print(f\"‚úÖ Successfully uploaded {len(agent_runs)} agent runs!\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to upload agent runs: {e}\")\n",
    "                upload_stats['failed_uploads'] += len(agent_runs)\n",
    "                for agent_run in agent_runs:\n",
    "                    upload_stats['failed_runs'].append({\n",
    "                        'agent_run_id': agent_run.id,\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "    \n",
    "    # Print final statistics\n",
    "    print(f\"\\nüìà Upload Statistics:\")\n",
    "    print(f\"   Total runs processed: {upload_stats['total_runs']}\")\n",
    "    print(f\"   Successfully uploaded: {upload_stats['successful_uploads']}\")\n",
    "    print(f\"   Failed uploads: {upload_stats['failed_uploads']}\")\n",
    "    print(f\"   Skipped runs: {upload_stats['skipped_runs']}\")\n",
    "    \n",
    "    if upload_stats['failed_runs']:\n",
    "        print(f\"   Failed runs: {len(upload_stats['failed_runs'])}\")\n",
    "        \n",
    "    return upload_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Looking for AssistantBench browser agent ZIP files...\n",
      "Found 12 AssistantBench browser agent ZIP files:\n",
      "   1. assistantbench_assistantbench_browser_agent_claude37sonnet20250219_1746383806_UPLOAD.zip (   826.7 MB)\n",
      "   2. assistantbench_assistantbench_browser_agent_claude37sonnet20250219_high_1748638033_UPLOAD.zip (   774.7 MB)\n",
      "   3. assistantbench_assistantbench_browser_agent_claudeopus4120250514_1754678912_UPLOAD.zip (  1522.9 MB)\n",
      "   4. assistantbench_assistantbench_browser_agent_claudeopus41_high_1755199872_UPLOAD.zip (  1464.0 MB)\n",
      "   5. assistantbench_assistantbench_browser_agent_deepseekaideepseekr1_1748894163_UPLOAD.zip (    17.7 MB)\n",
      "   6. assistantbench_assistantbench_browser_agent_deepseekaideepseekv3_1746233269_UPLOAD.zip (    58.5 MB)\n",
      "   7. assistantbench_assistantbench_browser_agent_gemini20flash_1755192000_UPLOAD.zip  (   635.9 MB)\n",
      "   8. assistantbench_assistantbench_browser_agent_gpt4120250414_1746225570_UPLOAD.zip  (   443.8 MB)\n",
      "   9. assistantbench_assistantbench_browser_agent_gpt5_1754633901_UPLOAD.zip           (  1059.0 MB)\n",
      "  10. assistantbench_assistantbench_browser_agent_o320250416_1750104946_UPLOAD.zip     (   590.7 MB)\n",
      "  11. assistantbench_assistantbench_browser_agent_o4mini20250416_high_1746297478_UPLOAD.zip (   727.1 MB)\n",
      "  12. assistantbench_assistantbench_browser_agent_o4mini20250416_low_1746374633_UPLOAD.zip (   597.9 MB)\n",
      "\n",
      "Ready to process 12 files...\n"
     ]
    }
   ],
   "source": [
    "# Find all matching ZIP files\n",
    "fs = HfFileSystem()\n",
    "repo_path = f\"datasets/{REPO_ID}@{REVISION}\"\n",
    "files = fs.ls(repo_path, detail=True)\n",
    "\n",
    "# Filter for AssistantBench browser agent files\n",
    "print(f\"\\nLooking for AssistantBench browser agent ZIP files...\")\n",
    "browser_agent_zip_files = []\n",
    "for file_info in files:\n",
    "    if file_info['name'].endswith('.zip'):\n",
    "        file_path = file_info['name']\n",
    "        file_name = file_path.split('/')[-1]\n",
    "        \n",
    "        # Only include files that start with \"assistantbench_assistantbench_browser_agent\"\n",
    "        if file_name.lower().startswith('assistantbench_assistantbench_browser_agent'):\n",
    "            # exclude a number of specific zip files names\n",
    "            excluded_files = [\n",
    "                'assistantbench_assistantbench_browser_agent_claude37sonnet20250219_low_1748711087_UPLOAD.zip',\n",
    "                'assistantbench_assistantbench_browser_agent_deepseekr1_1755121049_UPLOAD.zip',\n",
    "                'assistantbench_assistantbench_browser_agent_gemini20flash_1746393958_UPLOAD.zip',\n",
    "                'assistantbench_assistantbench_browser_agent_gpt5_1754598271_UPLOAD.zip',\n",
    "                'assistantbench_assistantbench_browser_agent_o320250416_1746376643_UPLOAD.zip',\n",
    "                'assistantbench_assistantbench_browser_agent_o4mini20250416_1746227177_UPLOAD.zip'\n",
    "\n",
    "            ]\n",
    "            if file_name not in excluded_files:\n",
    "                file_size = file_info.get('size', 0)\n",
    "                browser_agent_zip_files.append({\n",
    "                    'name': file_name,\n",
    "                    'size': file_size,\n",
    "                    'path': file_path\n",
    "            })\n",
    "\n",
    "# Sort by name for consistent processing\n",
    "browser_agent_zip_files.sort(key=lambda x: x['name'])\n",
    "\n",
    "print(f\"Found {len(browser_agent_zip_files)} AssistantBench browser agent ZIP files:\")\n",
    "for i, zf in enumerate(browser_agent_zip_files):\n",
    "    size_mb = zf['size'] / (1024 * 1024)\n",
    "    print(f\"  {i+1:2d}. {zf['name']:<80} ({size_mb:>8.1f} MB)\")\n",
    "\n",
    "print(f\"\\nReady to process {len(browser_agent_zip_files)} files...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing assistantbench_assistantbench_browser_agent_claude...:   8%|‚ñä         | 1/12 [00:32<05:56, 32.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ assistantbench_assistantbench_browser_agent_claude37sonnet20250219_1746383806_UPLOAD.zip: 10 tasks processed\n",
      "   Sample: Task efc0f3a47e9e... has 217 messages\n",
      "   Model: claude-3-7-sonnet-20250219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing assistantbench_assistantbench_browser_agent_claude...:  17%|‚ñà‚ñã        | 2/12 [00:59<04:51, 29.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ assistantbench_assistantbench_browser_agent_claude37sonnet20250219_high_1748638033_UPLOAD.zip: 10 tasks processed\n",
      "   Sample: Task efc0f3a47e9e... has 203 messages\n",
      "   Model: claude-3-7-sonnet-20250219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing assistantbench_assistantbench_browser_agent_claude...:  25%|‚ñà‚ñà‚ñå       | 3/12 [01:54<06:11, 41.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ assistantbench_assistantbench_browser_agent_claudeopus4120250514_1754678912_UPLOAD.zip: 10 tasks processed\n",
      "   Sample: Task efc0f3a47e9e... has 284 messages\n",
      "   Model: anthropic/claude-opus-4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing assistantbench_assistantbench_browser_agent_deepse...:  33%|‚ñà‚ñà‚ñà‚ñé      | 4/12 [02:43<05:54, 44.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ assistantbench_assistantbench_browser_agent_claudeopus41_high_1755199872_UPLOAD.zip: 10 tasks processed\n",
      "   Sample: Task 8ad84bd6fe38... has 361 messages\n",
      "   Model: anthropic/claude-opus-4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing assistantbench_assistantbench_browser_agent_deepse...:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 5/12 [02:45<03:23, 29.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ assistantbench_assistantbench_browser_agent_deepseekaideepseekr1_1748894163_UPLOAD.zip: 10 tasks processed\n",
      "   Sample: Task efc0f3a47e9e... has 28 messages\n",
      "   Model: deepseek-ai/DeepSeek-R1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing assistantbench_assistantbench_browser_agent_gemini...:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 6/12 [02:49<02:01, 20.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ assistantbench_assistantbench_browser_agent_deepseekaideepseekv3_1746233269_UPLOAD.zip: 10 tasks processed\n",
      "   Sample: Task efc0f3a47e9e... has 86 messages\n",
      "   Model: deepseek-ai/DeepSeek-V3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing assistantbench_assistantbench_browser_agent_gpt412...:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 7/12 [03:10<01:43, 20.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ assistantbench_assistantbench_browser_agent_gemini20flash_1755192000_UPLOAD.zip: 10 tasks processed\n",
      "   Sample: Task 929b45f34805... has 180 messages\n",
      "   Model: google/gemini-2.0-flash-001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing assistantbench_assistantbench_browser_agent_gpt5_1...:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 8/12 [03:25<01:15, 18.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ assistantbench_assistantbench_browser_agent_gpt4120250414_1746225570_UPLOAD.zip: 10 tasks processed\n",
      "   Sample: Task efc0f3a47e9e... has 119 messages\n",
      "   Model: gpt-4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing assistantbench_assistantbench_browser_agent_o32025...:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 9/12 [04:00<01:11, 23.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ assistantbench_assistantbench_browser_agent_gpt5_1754633901_UPLOAD.zip: 10 tasks processed\n",
      "   Sample: Task efc0f3a47e9e... has 282 messages\n",
      "   Model: gpt-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing assistantbench_assistantbench_browser_agent_o4mini...:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 10/12 [04:19<00:44, 22.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ assistantbench_assistantbench_browser_agent_o320250416_1750104946_UPLOAD.zip: 10 tasks processed\n",
      "   Sample: Task 6b06d186921b... has 152 messages\n",
      "   Model: o3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing assistantbench_assistantbench_browser_agent_o4mini...:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 11/12 [04:41<00:22, 22.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ assistantbench_assistantbench_browser_agent_o4mini20250416_high_1746297478_UPLOAD.zip: 10 tasks processed\n",
      "   Sample: Task efc0f3a47e9e... has 248 messages\n",
      "   Model: o4-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing assistantbench_assistantbench_browser_agent_o4mini...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [05:00<00:00, 25.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ assistantbench_assistantbench_browser_agent_o4mini20250416_low_1746374633_UPLOAD.zip: 10 tasks processed\n",
      "   Sample: Task 4e615af6f034... has 26 messages\n",
      "   Model: o4-mini\n",
      "\n",
      "üéâ Processing complete!\n",
      "Total files processed: 12\n",
      "Successful files: 12\n",
      "Total tasks extracted: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process all browser agent files and collect results\n",
    "all_results = {}  # zip_name -> {task_id -> agent_run}\n",
    "overall_progress = tqdm(browser_agent_zip_files, desc=\"Processing ZIP files\")\n",
    "\n",
    "for file_info in overall_progress:\n",
    "    zip_name = file_info['name']\n",
    "    overall_progress.set_description(f\"Processing {zip_name[:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Process this ZIP file\n",
    "        zip_results = {}\n",
    "        \n",
    "        for tid, agent_run in stream_agent_runs_by_task(\n",
    "            repo_id=REPO_ID,\n",
    "            zip_name=zip_name,\n",
    "            revision=REVISION,\n",
    "            repo_kind=\"datasets\",\n",
    "            member_name=None,\n",
    "            require_model=None,\n",
    "            include_eval=True,\n",
    "            limit=10,  # Process all tasks\n",
    "            aggregate_all=True,\n",
    "        ):\n",
    "            zip_results[tid] = agent_run\n",
    "        \n",
    "        all_results[zip_name] = zip_results\n",
    "        \n",
    "        print(f\"\\n‚úÖ {zip_name}: {len(zip_results)} tasks processed\")\n",
    "        \n",
    "        # Show sample info\n",
    "        if zip_results:\n",
    "            sample_task_id = next(iter(zip_results))\n",
    "            sample_run = zip_results[sample_task_id]\n",
    "            print(f\"   Sample: Task {sample_task_id[:12]}... has {len(sample_run.get('messages', []))} messages\")\n",
    "            if sample_run.get('model'):\n",
    "                print(f\"   Model: {sample_run['model']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error processing {zip_name}: {e}\")\n",
    "        all_results[zip_name] = {\"error\": str(e)}\n",
    "        continue\n",
    "\n",
    "overall_progress.close()\n",
    "\n",
    "print(f\"\\nüéâ Processing complete!\")\n",
    "print(f\"Total files processed: {len(all_results)}\")\n",
    "successful_files = sum(1 for v in all_results.values() if \"error\" not in v)\n",
    "print(f\"Successful files: {successful_files}\")\n",
    "total_tasks = sum(len(v) for v in all_results.values() if \"error\" not in v)\n",
    "print(f\"Total tasks extracted: {total_tasks:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the conversion\n",
    "docent_results, stats = convert_to_docent_messages(all_results)\n",
    "\n",
    "print(f\"\\n‚úÖ Conversion complete!\")\n",
    "print(f\"üìä Message conversion stats:\")\n",
    "print(f\"   Total messages: {stats['total_messages']}\")\n",
    "print(f\"   Successfully converted: {stats['successful']}\")\n",
    "print(f\"   Failed to convert: {stats['failed']}\")\n",
    "if stats['total_messages'] > 0:\n",
    "    print(f\"   Success rate: {stats['successful']/stats['total_messages']*100:.1f}%\")\n",
    "\n",
    "if stats['error_types']:\n",
    "    print(f\"\\n‚ö†Ô∏è  Error types:\")\n",
    "    for error_type, count in stats['error_types'].items():\n",
    "        print(f\"   {error_type}: {count}\")\n",
    "\n",
    "if stats['failed_tasks']:\n",
    "    print(f\"\\nüìã Tasks with failed messages: {len(stats['failed_tasks'])}\")\n",
    "    for task_id, failed_count in stats['failed_tasks'][:5]:  # Show first 5\n",
    "        print(f\"   {task_id[:20]}...: {failed_count} failed messages\")\n",
    "\n",
    "print(f\"\\nüìÅ Converted {sum(len(tasks) for tasks in docent_results.values() if 'error' not in tasks)} agent runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example usage with model batching:\ndocent_api_key = os.getenv(\"DOCENT_API_KEY\")\nif not docent_api_key:\n    raise ValueError(\"DOCENT_API_KEY environment variable is required\")\n\nclient = Docent(\n    api_key=docent_api_key,\n)\n\ncollection_id = client.create_collection(\n    name=\"assistantbench_batched\",\n    description=\"Testing integration of Hal and Docent with AssistantBench (batched by model)\",\n)\n\nupload_stats = upload_all_transcripts_to_transluce(docent_results, collection_id, client, batch_by_model=True)"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved complete results to: assistantbench_browser_agent_runs.json\n",
      "üìä File size: 17.5 MB\n",
      "üìã Contains 12 ZIP files with 120 total tasks\n"
     ]
    }
   ],
   "source": [
    "# Save results to JSON file\n",
    "import json, base64\n",
    "from datetime import date, datetime\n",
    "from decimal import Decimal\n",
    "\n",
    "def _json_default(o):\n",
    "    if isinstance(o, Decimal):\n",
    "        return format(o, \"f\")  # keep exact value as a string\n",
    "    if isinstance(o, (datetime, date)):\n",
    "        return o.isoformat()\n",
    "    if isinstance(o, bytes):\n",
    "        return base64.b64encode(o).decode(\"ascii\")\n",
    "    # optional: numpy support\n",
    "    try:\n",
    "        import numpy as np\n",
    "        if isinstance(o, (np.integer, np.floating, np.bool_)):\n",
    "            return o.item()\n",
    "        if isinstance(o, np.ndarray):\n",
    "            return o.tolist()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return str(o)\n",
    "\n",
    "output_file = \"assistantbench_browser_agent_runs.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2, default=_json_default, sort_keys=True)\n",
    "\n",
    "file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "print(f\"‚úÖ Saved complete results to: {output_file}\")\n",
    "print(f\"üìä File size: {file_size_mb:.1f} MB\")\n",
    "print(f\"üìã Contains {len(all_results)} ZIP files with {sum(len(v) for v in all_results.values() if 'error' not in v)} total tasks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}